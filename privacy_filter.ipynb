{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiYr3ymNTuPEfU4+czHKCU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyAgarwal11/Privacy-Preserving-Representation-for-Audio-Visual-Speech-Understanding/blob/main/privacy_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preliminaries**"
      ],
      "metadata": {
        "id": "qBmn_DkCFAFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "JxlYZMCuNefB",
        "outputId": "db3961ce-cae4-4820-ad4f-ec0a75fa40fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GABUf90oBGy3",
        "outputId": "2d2408ca-cd86-4b70-d3d6-197e0ece2f45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Privacy-Preserving-Representation-for-Audio-Visual-Speech-Understanding'...\n",
            "remote: Enumerating objects: 103711, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 103711 (delta 2), reused 0 (delta 0), pack-reused 103705\u001b[K\n",
            "Receiving objects: 100% (103711/103711), 2.95 GiB | 38.23 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Updating files: 100% (103015/103015), done.\n",
            "/content\n",
            "fatal: destination path 'av_hubert' already exists and is not an empty directory.\n",
            "/content/av_hubert\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting python_speech_features\n",
            "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: python_speech_features\n",
            "  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5870 sha256=ae653e06a28b22f715aac8be6629b826c76df5dd1874fa4b53b6ad156448e57e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n",
            "Successfully built python_speech_features\n",
            "Installing collected packages: python_speech_features\n",
            "Successfully installed python_speech_features-0.6\n",
            "Collecting scikit-video\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from scikit-video) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.11.4)\n",
            "Installing collected packages: scikit-video\n",
            "Successfully installed scikit-video-1.1.11\n",
            "/content/av_hubert/fairseq\n",
            "Processing /content/av_hubert/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+afc77bd) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+afc77bd) (3.0.10)\n",
            "Collecting hydra-core<1.1 (from fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+afc77bd) (2023.12.25)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+afc77bd) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+afc77bd) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+afc77bd) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+afc77bd) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+afc77bd) (4.10.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+afc77bd) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+afc77bd) (4.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==1.0.0a0+afc77bd) (2.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+afc77bd) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+afc77bd) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+afc77bd) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+afc77bd) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+afc77bd) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+afc77bd) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq==1.0.0a0+afc77bd)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq==1.0.0a0+afc77bd) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq==1.0.0a0+afc77bd) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+afc77bd-cp310-cp310-linux_x86_64.whl size=2472386 sha256=a900a6ebf10157e64f5aaa6290215f81e6ef6a921eed5c2347273fa141b1432c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ws6cb8kt/wheels/9d/d5/16/2858bd41b3c8f8a9994060d9742bf0c2277ddbd72d53c55737\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=2a8b64417ff5bf27018f2d6111d7caffc5dd253e840fe6c8697720039efae317\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, portalocker, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 colorama-0.4.6 fairseq-1.0.0a0+afc77bd hydra-core-1.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ShreyAgarwal11/Privacy-Preserving-Representation-for-Audio-Visual-Speech-Understanding.git\n",
        "%cd /content/\n",
        "!git clone https://github.com/facebookresearch/av_hubert.git\n",
        "\n",
        "%cd av_hubert\n",
        "!git submodule init\n",
        "!git submodule update\n",
        "!pip install scipy\n",
        "!pip install sentencepiece\n",
        "!pip install python_speech_features\n",
        "!pip install scikit-video\n",
        "\n",
        "%cd fairseq\n",
        "!pip install ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data/misc/\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 -O /content/data/misc/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -d /content/data/misc/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!wget --content-disposition https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks/raw/master/preprocessing/20words_mean_face.npy -O /content/data/misc/20words_mean_face.npy"
      ],
      "metadata": {
        "id": "Z8VUJ4WLkHoK",
        "outputId": "e7f11922-9e3f-43a8-a045-d12214c9d318",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-04 22:47:03--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘/content/data/misc/shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "/content/data/misc/ 100%[===================>]  61.07M  17.2MB/s    in 3.9s    \n",
            "\n",
            "2024-04-04 22:47:07 (15.5 MB/s) - ‘/content/data/misc/shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n",
            "--2024-04-04 22:47:16--  https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks/raw/master/preprocessing/20words_mean_face.npy\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks/master/preprocessing/20words_mean_face.npy [following]\n",
            "--2024-04-04 22:47:16--  https://raw.githubusercontent.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks/master/preprocessing/20words_mean_face.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1168 (1.1K) [application/octet-stream]\n",
            "Saving to: ‘/content/data/misc/20words_mean_face.npy’\n",
            "\n",
            "/content/data/misc/ 100%[===================>]   1.14K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-04 22:47:16 (86.7 MB/s) - ‘/content/data/misc/20words_mean_face.npy’ saved [1168/1168]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import a pre-trained model**"
      ],
      "metadata": {
        "id": "B1Mx4qTIG1AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tuned model -> Noise-Augmented AV-HuBERT Base"
      ],
      "metadata": {
        "id": "k9u9TinFIF3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "%mkdir -p /content/data/\n",
        "!wget https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/avsr/base_noise_pt_noise_ft_433h.pt -O /content/data/finetune-model.pt"
      ],
      "metadata": {
        "id": "1e8mNAjvFS9U",
        "outputId": "055e59fd-91a7-4ad7-bd39-9299c269e4a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/av_hubert/fairseq\n",
            "--2024-04-04 22:47:16--  https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/avsr/base_noise_pt_noise_ft_433h.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.108, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1928060481 (1.8G) [binary/octet-stream]\n",
            "Saving to: ‘/content/data/finetune-model.pt’\n",
            "\n",
            "/content/data/finet 100%[===================>]   1.79G   117MB/s    in 11s     \n",
            "\n",
            "2024-04-04 22:47:28 (162 MB/s) - ‘/content/data/finetune-model.pt’ saved [1928060481/1928060481]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Video Out of Frames**"
      ],
      "metadata": {
        "id": "vYrFJ2tuZloT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "frame_folder = '/content/Privacy-Preserving-Representation-for-Audio-Visual-Speech-Understanding/VidTIMIT/fadg0/video/sa1'\n",
        "\n",
        "output_video_path = '/content/output_video.mp4'\n",
        "\n",
        "frame_rate = 25\n",
        "\n",
        "frame_files = [f for f in os.listdir(frame_folder) if os.path.isfile(os.path.join(frame_folder, f))]\n",
        "\n",
        "frame_files.sort()\n",
        "\n",
        "video_resolution = (512, 384)\n",
        "\n",
        "if video_resolution is None:\n",
        "    first_frame_path = os.path.join(frame_folder, frame_files[0])\n",
        "    first_frame = cv2.imread(first_frame_path)\n",
        "    video_resolution = (first_frame.shape[1], first_frame.shape[0])\n",
        "\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, video_resolution)\n",
        "\n",
        "for frame_file in frame_files:\n",
        "    frame_path = os.path.join(frame_folder, frame_file)\n",
        "    frame = cv2.imread(frame_path)\n",
        "    if (frame.shape[1], frame.shape[0]) != video_resolution:\n",
        "        frame = cv2.resize(frame, video_resolution)\n",
        "    out.write(frame)\n",
        "\n",
        "out.release()\n"
      ],
      "metadata": {
        "id": "91haYEVIZtYa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/av_hubert/avhubert\n",
        "import cv2\n",
        "import tempfile\n",
        "import torch\n",
        "import utils as avhubert_utils\n",
        "from argparse import Namespace\n",
        "import fairseq\n",
        "from fairseq import checkpoint_utils, options, tasks, utils\n",
        "from IPython.display import HTML\n",
        "from python_speech_features import logfbank\n",
        "from scipy.io import wavfile"
      ],
      "metadata": {
        "id": "LjBk19NqG-X0",
        "outputId": "5c87d740-c06c-4e94-eb95-a1f3451bdf54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/av_hubert/avhubert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction using AV-HUBERT**"
      ],
      "metadata": {
        "id": "LZm64rT1hyRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stacker(feats, stack_order):\n",
        "            \"\"\"\n",
        "            Concatenating consecutive audio frames\n",
        "            Args:\n",
        "            feats - numpy.ndarray of shape [T, F]\n",
        "            stack_order - int (number of neighboring frames to concatenate\n",
        "            Returns:\n",
        "            feats - numpy.ndarray of shape [T', F']\n",
        "            \"\"\"\n",
        "            feat_dim = feats.shape[1]\n",
        "            if len(feats) % stack_order != 0:\n",
        "                res = stack_order - len(feats) % stack_order\n",
        "                res = np.zeros([res, feat_dim]).astype(feats.dtype)\n",
        "                feats = np.concatenate([feats, res], axis=0)\n",
        "            feats = feats.reshape((-1, stack_order, feat_dim)).reshape(-1, stack_order*feat_dim)\n",
        "            return feats"
      ],
      "metadata": {
        "id": "KVK2SUGkWZED"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_visual_feature(video_path, audio_path, ckpt_path, user_dir, is_finetune_ckpt=False):\n",
        "  utils.import_user_module(Namespace(user_dir=user_dir))\n",
        "  models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])\n",
        "  transform = avhubert_utils.Compose([\n",
        "      avhubert_utils.Normalize(0.0, 255.0),\n",
        "      avhubert_utils.CenterCrop((task.cfg.image_crop_size, task.cfg.image_crop_size)),\n",
        "      avhubert_utils.Normalize(task.cfg.image_mean, task.cfg.image_std)])\n",
        "  frames = avhubert_utils.load_video(video_path)\n",
        "  print(f\"Load video {video_path}: shape {frames.shape}\")\n",
        "  sample_rate, wav_data = wavfile.read(audio_path)\n",
        "  audio_features = logfbank(wav_data, sample_rate).astype(np.float32)\n",
        "  audio_features = stacker(audio_features, 4)\n",
        "  print(f\"Load audio {audio_path}: shape {audio_features.shape}\")\n",
        "  audio_features = torch.FloatTensor(audio_features).unsqueeze(dim=0).permute(0, 2, 1).cuda()\n",
        "  frames = torch.FloatTensor(frames).unsqueeze(dim=0).unsqueeze(dim=0).cuda()\n",
        "  model = models[0]\n",
        "  if hasattr(models[0], 'decoder'):\n",
        "    print(f\"Checkpoint: fine-tuned\")\n",
        "    model = models[0].encoder.w2v_model\n",
        "  else:\n",
        "    print(f\"Checkpoint: pre-trained w/o fine-tuning\")\n",
        "  model.cuda()\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # Specify output_layer if you want to extract feature of an intermediate layer\n",
        "    feature1, _ = model.extract_finetune(source={'video': frames, 'audio': audio_features}, padding_mask=None, output_layer=1)\n",
        "    feature2, _ = model.extract_finetune(source={'video': frames, 'audio': audio_features}, padding_mask=None, output_layer=13)\n",
        "    feature, _ = model.extract_finetune(source={'video': frames, 'audio': audio_features}, padding_mask=None, output_layer=None)\n",
        "    feature = feature.squeeze(dim=0)\n",
        "    print(feature1, feature2, feature)\n",
        "  print(f\"Video feature shape: {feature.shape}\")\n",
        "  return feature1, feature\n",
        "\n",
        "mouth_roi_path, ckpt_path = \"/content/output_video.mp4\", \"/content/data/finetune-model.pt\"\n",
        "audio_path = \"/content/Privacy-Preserving-Representation-for-Audio-Visual-Speech-Understanding/VidTIMIT/fadg0/audio/sa1.wav\"\n",
        "user_dir = \"/content/av_hubert/avhubert\"\n",
        "feature1, feature = extract_visual_feature(mouth_roi_path, audio_path, ckpt_path, user_dir)"
      ],
      "metadata": {
        "id": "C6RNFXKwIfqN",
        "outputId": "2f0923dd-533f-457c-e2a7-a8aa50fd4374",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:frame length (800) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load video /content/output_video.mp4: shape (119, 384, 512)\n",
            "Load audio /content/Privacy-Preserving-Representation-for-Audio-Visual-Speech-Understanding/VidTIMIT/fadg0/audio/sa1.wav: shape (119, 104)\n",
            "Checkpoint: fine-tuned\n",
            "tensor([[[-0.7051,  0.0766, -0.3617,  ...,  0.6418,  1.1220,  2.0583],\n",
            "         [-0.3687,  0.1774,  0.0529,  ..., -0.1324,  0.6845,  1.6880],\n",
            "         [-0.3288,  0.2121,  0.2088,  ..., -0.2046,  0.9320,  1.1184],\n",
            "         ...,\n",
            "         [-0.4207,  0.1412,  0.1495,  ...,  0.2004,  0.7020, -0.1858],\n",
            "         [-0.5896,  0.4344,  0.0917,  ...,  0.5695,  1.0895, -0.4366],\n",
            "         [ 0.5591,  0.4939,  1.2549,  ...,  0.7196,  1.8432, -0.0850]]],\n",
            "       device='cuda:0') tensor([[[ -3.7520, -28.4042,   0.6904,  ...,  46.7078,  12.6127,  34.2598],\n",
            "         [ 21.7954,  -2.8649,   9.3908,  ..., -29.0639,  36.4860,   6.6163],\n",
            "         [ 13.3123,   6.8865,  14.1806,  ..., -38.9309,  47.3546, -24.9597],\n",
            "         ...,\n",
            "         [-34.1372,  20.4861,  84.1487,  ..., -33.2451, -11.2398, -38.1866],\n",
            "         [-55.2007,  13.7286, 100.5256,  ..., -13.1866,   4.8790, -43.9628],\n",
            "         [-56.3256,  14.4793,  89.1961,  ...,  15.0271,   7.8770, -49.7655]]],\n",
            "       device='cuda:0') tensor([[ 0.0114, -0.1456, -0.0193,  ...,  0.2334,  0.0259,  0.1726],\n",
            "        [ 0.1265, -0.0513,  0.0219,  ..., -0.1136,  0.1439,  0.0935],\n",
            "        [ 0.0944, -0.0028,  0.0492,  ..., -0.1800,  0.2098, -0.0352],\n",
            "        ...,\n",
            "        [-0.1241,  0.0583,  0.3522,  ..., -0.1326, -0.0627, -0.0778],\n",
            "        [-0.2085,  0.0221,  0.3987,  ..., -0.0259,  0.0050, -0.0929],\n",
            "        [-0.2096,  0.0205,  0.3384,  ...,  0.1089,  0.0136, -0.1118]],\n",
            "       device='cuda:0')\n",
            "Video feature shape: torch.Size([119, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iD4ylRKRbRyu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}